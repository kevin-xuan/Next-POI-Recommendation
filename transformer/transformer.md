## Attention Is All You Need
### Methodology
The author proposes a new simple network architecture based solely on attention mechanism: **Transformer**. It follows this overall framework using stacked **self-attention** and **point-wise fully connected** layers for both the **encoder** and **decoder**.

* **Encoder**: It consists of a stack of several identical layers, and each layer is composed of two sub-layers: **multi-head attention layer** and **position-wise fully connected feed-forward network**. Besides, a **residual connection** is employed around each of the two sub-layers, followed by layer normalization.
Specifically, **multi-head attention layer** utilizes **different** learned **linear projections** to project three inputs: **query, key, and value**, which means the model could caputure information from **different subspaces**. And **position-wise fully connected feed-forward network** feeds the **output** of multi-head attention layer into two fully connected layers, the first of which uses **ReLu** activation function. 

* **Decoder**: It also consists of a stack of several identical layers, but each layer consists of three sub-layers: **masked multi-head attention layer**, **multi-head attention layer**, and **position-wise fully connected feed-forward network**. Likewise, a **residual connection** is employed around each of the three sub-layers, followed by layer normalization. However, the three multi-head attention layers are different from each other. In particular, the multi-head attention regards the **outputs of previous layer** as its inputs in the encoder. The inputs of **masked multi-head attention** are from the previous layer in the decoder, and it is used to prevent leftward information flow in the decoder to perserve the auto-regressive property. In other words, when predicting **target word** in current time step, we could only use those **known information** from previous time step to make prediction. The **future information** should not be used and thus need to be masked. Moreover, the inputs of multi-head attention are from **encoder layer and decoder layer** in the decoder. That is, **key** and **value** are from encoder layer, but **query** is from previous decoder layer. Therefore, it is also called **interaction sub-layer**. Note that the output of the encoder will be sent to the interaction sub-layer of each layer of the decoder.
* **Prediction**: the output of decoder will be sent to a linear transformation followed by softmax function to get the predicted next-tokens probabilities.


We have discussed the overall framework of **Transformer**, the implementation details will be introduced now, which is composed of **scaled dot-product attention**, **position embedding**.

* **Scaled dot-product attention**: The two most commonly used attention functions are **additive attention** and **dot-product attention**. The **dot-product attention** is much faster and more space-efficient in practice. One of the inputs of the two attention functions is **query**. Experiments have shown that the two attention functions achieve similar effects when the **dimension** of query is small. But, when the **dimension** is large, additive attention outperforms dot-product attention. The author uses a simple example to illustrate why dot products grow large in magnitude when the **dimension** is large. In the example, dot products has **large variance**, which is equal to **dimension**, pushing the softmax function into regions where it has extremely small gradients. As a result, a scaled dot-prodcuct attention is employed to make the **variance** 1.
* **Position embedding**: **Input embedding** and **output embedding** are used to convert the input and output tokens to **vectors** with same dimension. These vectors are fed into **self-attention** for parallel computation. However, these vectors contain no information about the **relative or absolute position** of the corresponding tokens in the sequence. Therefore, we add **position embedding** to the **input embedding** and **output embedding** at the bottoms of the encoder and decoder stacks in order to make model take advantage of the order of sequence. Specifically, we use **sine** and **cosine** function of different frequencies, **each dimension** of the positional encoding corresponds to a sunusoid. That is, **"even"** dimension is **sine** value, and **"odd"** dimension is **cosine** vaule. In this way, it would allow the model to easily learn to attend by relative positions, which means that **absolute position embedding** contains information about **relative position embedding**. Besides, it may allow the model to extrapolate to sequence lengths longer than ones encountered during training.


### Attention
#### Soft Attention
The conventional **Seq2Seq** model employs **encoder-decoder** framework to perform some sequential tasks, such as machine translation. The **encoder** module is responsible for encoding each word in **source** sentence to get **semantic** information, which is used to predict **target** sentence. In this way, the **decoder** module utilizes the **same** semantic information to predict each **word** in the **target** sentence. In other words, each word in **source** sentence has a same impact on the **predicted word** in **target** sentence. Obviously, it is more reasonable that each word in **source** sentence should have a different effect on the **predicted** word.

Therefore, people introduce **Attention** mechanism, which is used to replace the **same semantic** with **variable** semantic by assigning different **attention** to each word in **source** sentence for predicting the **current word** in **target** sentence.
Specifically, when predicting target word in current time step, we use a **similarity function** to calculate the similarity between hidden state of **previous time step** and hidden state of **each word** in **source** sentence. The similarity means **attention**, which reflects the importance on **target word**.

In general, we can image each word in **source** sentence into a **"key,value"** pair (key==value) and some **target word** into a **query**. Then we could regard the similarity (**softmax** is used to normalize the similarity) between each **key** and **query** as weight coefficient of corresponding **value**. The final **Attention** is the weighted sum of each **value**.

#### Self Attention
Above **Soft Attenion** is used in two different sentences: **source** and **target** sentence. It means that **Soft Attention** aims to reveal some regularities between some **target word** and all words in **source** sentence. However, the goal of **Self Attention** is to extract the regularities between words within **source** sentence or **target** sentence, that is, **source** sentence is same as **target** sentence. It is easier to learn long-range dependices than RNN or LSTM by introduction Self Attention. Because **Self Attention** could compute directly the relationship between two arbitrary words in the sentence. However, as for RNN or LSTM, they have to calculate the relationship in sequential order, the longer the distance between two arbitraty words, the harder it is to learn such long-range dependencies. Besides, **Self Attention** is helpful to parallel computation.