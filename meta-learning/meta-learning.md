## MAMO: Memory-Augmented Meta-Optimization for Cold-start Recommendation

### Meta-Learning
Meta-learning, i.e., learning to learn, means that learning the experience between some similar tasks to fastly adapt to new task with few traing samples. In this paper, the recommendation for each user is regarded as a learning task. The **core idea** is learning a global paramater to initialize the parameter of **personalized recommender models**. The **personalized parameter** will be **locally updated** to learn a **specific user's** preference, and the **global parameter** will be **globally updated** by **minimizing the loss** over the training tasks among **all users**. Then the learned **global parameter** will be used to guide model settings for new user.

### Innovation
Most meta-learning based recommendation approaches adopt **model-agnostic meta-learning(MAML)** for parameter initialization, which suffer from a variety of issues such as instability, slow convergence, and weak generalization. Especially, they use the **sharing global parameter** to initialize personalized parameter of all users, which may lead model into **local optima** for some users.

To address this issue, the paper designs **feature-specific memory** and **task-specific memory** cube, respectively. Specifically, the **feature-specific memory** provides a **personalized bias** when initializing the model parameter. And the **task-specific memory** cube, i.e., **user preference memory**, is used to learn the shared potential user preference commonality on different items. The paper proposes an adaptive meta-learning recommendation model, **Memory-Augmented Meta-Optimization(MAMO)**, to improve stability and generalization by learning a **multi-level** personalized model parameters.

### Methodology
The proposed model includes two parts: the recommender model and the memory-augmented meta-optimization learner.

* **recommender model**: predict the user preference.
* **meta-optimization learner**: initialize the recommender model parameter.

#### Recommender Model
First, we use two multi-layer fully connected layers to learn the **user and item embedding vector** by user and item **profile**, respectively. Then a matrix containing **weights preference information** about the user is extraced from the **task-specific memory** according to the **user profile**. Finally, the **matrix** and the **concatenation** of **user embedding and item embedding** are fed into a multi-layer fully connected layer to get the prediction of preference score for each item.

#### Memory-Augmented Meta-Optimization

##### Feature-specific Memory
It consists of two parts: **profile memory** and **user embedding memory**. Specifically, **profile memory** provides information revelant to **user profile**. In other words, it stores several **different user types** information. And **user embedding memory** store the **fast gradients** of corresponding user types.

 **Attention mechanism** is adopted to get the retrieval attention vaules. Specifically, first, we use **consine** function to compute the similarity between **user profile** and each row of **profile memory**, and then the similarity is normalized by **softmax** function. Finally, retrieval attention vaules is the weighted sum of each row in **profile memory**. 
 
The calculated **attention vaules** are applied for extracting **bias** from **user embedding memory**.
In this way, the generated **bias** item could be used to initialize **personalized** parameter.

##### Task-specific Memory
The user preference matrix is generated by attentively retrieving **task-specific memory**. That is, we regard the **cross product** between above mentioned **attention values** and **task-specific memory** as the user preference matirx.

